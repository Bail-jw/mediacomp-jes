% intromediacomp.tex, first chapter of "Introduction to Media Computation"	\graphicspath{{figs/}{figs/intromediacomp/}}%What is computer science?	\section{What is computer science about?}	Computer science is the study of \emph{process}: How we do things, how we specify what we do, how we specify what the stuff is that you're processing.  But that's a pretty dry definition.  Let's try a metaphorical one.\label{pg:process}\begin{csconcept}{Computer science is the study of recipes}\index{recipe}\index{computational recipe} They're a special kind of recipe---one that can be executed by a computational device, but that point is only of importance to computer scientists.  The important point overall is that a computer science recipe defines \emph{exactly} what's to be done. \end{csconcept}If you're a biologist who wants to describe how migration works or how DNA replicates, or if you're a chemist who wants to explain how an equilibrium is reached in a reaction, or if you're a factory manager who wants to define a machine-and-belt layout and even test how it works before physically moving heavy things into position, then being able to write a recipe that specifies \emph{exactly} what happens, in terms that can be completely defined and understood, is \emph{very} useful.  This exactness is part of why computers have radically changed so much of how science is done and understood.%It's about recipes, and computer scientists study different kinds of recipes (argument from slides)	It may sound funny to call \emph{programs}\index{program} or \emph{algorithms}\index{algorithm} a recipe, but the analogy goes a long way.  Much of what computer scientists study can be defined in terms of recipes:\begin{itemize}\item Some computer scientists study how recipes are written: Are there better or worse ways of doing something?  If you've ever had to separate whites from yolks in eggs, you know that knowing the right way to do it makes a world of difference.  Computer science theoreticians worry about the fastest and shortest recipes, and the ones that take up the least amount of space (you can think about it as counter space --- the analogy works).  \emph{How} a recipe works, completely apart from how it's written, is called the study of \newterm{algorithms}. \index{software engineering}Software engineers worry about how large groups can put together recipes that still work.  (The recipe for some programs, like the one that keeps track of Visa/MasterCard records has literally millions of steps!)\item Other computer scientists study the units used in recipes.  Does it matter whether a recipe uses metric or English measurements?  The recipe may work in either case, but if you have the read the recipe and you don't know what a pound or a cup is, the recipe is a lot less understandable to you.  There are also units that make sense for some tasks and not others, but if you can fit the units to the tasks well, you can explain yourself more easily and get things done faster---and avoid errors.  Ever wonder why ships at sea measure their speed in \newterm{knots}?  Why not use things like meters per second?  There are places, like at sea, where more common terms aren't appropriate or don't work as well.  The study of computer science units is referred to as \newterm{data structures}.  Computer scientists who study ways of keeping track of lots of data in lots of different kinds of units are studying \newterm{databases}.\item Can recipes be written for anything?  Are there some recipes that \emph{can't} be written?  Computer scientists actually do know that there are recipes that can't be written.  For example, you can't write a recipe that can absolutely tell, for any other recipe, if the other recipe will actually work.  How about \newterm{intelligence}?  Can we write a recipe that can \emph{think} (and how would you tell if you got it right)?  Computer scientsts in \newterm{theory}, \newterm{intelligent systems}, \newterm{artificial intelligence}, and \newterm{systems} worry about things like this.\item There are even computer scientists who worry about whether people like what the recipes produce, like the restauraunt critics for the newspaper.  Some of these are \newterm{human-computer interface} specialists who worry about whether people like how the recipes work (those ``recipes'' that produce an \newterm{interface} that people use, like windows, buttons, scrollbars, and other elements of what we think about as a running program).\item Just as some chefs specialize in certain kinds of recipes, like crepes or barbeque, computer scientists also specialize in special kinds of recipes.  Computer scientists who work in \newterm{graphics} are mostly concerned with recipes that produce pictures, animations, and even movies.  Computer scientists who work in \newterm{computer music} are mostly concerned with recipes that produce sounds (often melodic ones, but not always).\item Still other computer scientists study the \newterm{emergent properties} of recipes.  Think about the World Wide Web.  It's really a collection of \emph{millions} of recipes (programs) talking to one another.  Why would one section of the Web get slower at some point?  It's a phenomena that emerges from these millions of programs, certainly not something that was planned.  That's something that \newterm{networking} computer scientists study.  What's really amazing is that these emergent properties (that things just start to happen when you have many, many recipes interacting at once) can also be used to explain non-computational things.  For example, how ants forage for food or how termites make mounds can also be described as something that just happens when you have lots of little programs doing something simple and interacting. \end{itemize}The recipe metaphor also works on another level.  Everyone knows that some things in recipe can be changed without changing the result dramatically.  You can always increase all the units by a multiplier to make more.  You can always add more garlic or oregano to the spaghetti sauce.  But there are some things that you cannot change in a recipe.  If the recipe calls for baking powder, you may not substitute baking soda.  If you're supposed to boil the dumplings then saute' them, the reverse order will probably not work well.Similarly, for software recipes.  There are usually things you can easily change: The actual names of things (though you should change names consistently), some of the \newterm{constants} (numbers that appear as plain old numbers, not as variables), and maybe even some of the data \newterm{ranges} (sections of the data) being manipulated.  But the order of the commands to the computer, however, almost always has to stay exactly as stated.  As we go on, you'll learn what can be changed safely, and what can't.%Programming languages are different ways to express recipes	Computer scientists specify their recipes with \newterm{programming languages}.  Different programming languages are used for different purposes.  Some of them are wildly popular, like Java and C++.  Others are more obscure, like Squeak and T.  Others are designed to make computer science ideas very easy to learn, like Scheme or Python, but the fact that they're easy to learn doesn't always make them very popular nor the best choice for experts building larger or more complicated recipes.  It's a hard balance in teaching computer science to pick a language that is easy to learn \emph{and} is popular and useful enough that students are motivated to learn it.Why don't computer scientists just use natural languages, like English or Spanish?  The problem is that natural languages evolved the way that they did to enhance communications between very smart beings, humans.  As we'll go into more in the next section, computers are exceptionally dumb.  They need a level of specificity that natural language isn't good at.  Further, what we say to one another in natural communication is not exactly what you're saying in a computational recipe.  When was the last time you told someone how a videogame like Doom or Quake or Super Mario Brothers worked in such minute detail that they could actually replicate the game (say, on paper)?  English isn't good for that kind of task.There are so many different kinds of programming languages because there are so many different kinds of recipes to write.  Programs written in the programming language \newterm{C} tend to be very fast and efficient, but they also tend to be hard to read, hard to write, and require units that are more about computers than about bird migrations or DNA or whatever else you want to write your recipe about.  The programming language \newterm{Lisp} (and its related languages like Scheme, T, and Common Lisp) is very flexible and is well suited to exploring how to write recipes that have never been written before, but Lisp \emph{looks} so strange compared to languages like C that many people avoid it and there are (natural consequence) few people who know it.  If you want to hire a hundred programmers to work on your project, you're going to find it easier to find a hundred programmers who know a popular language than a less popular one---but that doesn't mean that the popular language is the best one for your task!The programming language that we're using in this book is \newterm{Python} (\url{http://www.python.org}  for more information on Python).  Python is a fairly popular programming language, used very often for Web and media programming.  The web search engine \newterm{Google} is mostly programmed in Python.  The media company \newterm{Industrial Light \& Magic} also uses Python.  Python is known for being easy to learn, easy to read, very flexible, but not very efficient.  The same algorithm coded in C and in Python will probably be faster in C.  The version of Python that we're using is called \newterm{Jython} (\url{http://www.jython.org}).  Python is normally implemented in the programming language C.  Jython is Python implemented in Java.  Jython lets us do multimedia that will work across multiple computer platforms.\section{What Computers Understand} % What computers are about	\label{sec:encodings} Computational recipes are written to run on computers\index{computer}.  What does a computer know how to do?  What can we tell the computer to do in the recipe?  The answer is ``Very, very little.''  Computers are exceedingly stupid.  They really only know about numbers.Actually, even to say that computers know numbers is a myth, or more appropriately, an \newterm{encoding}.  Computers are electronic devices that react to voltages on wires.  We group these wires into sets (like eight of these wires are called a \newterm{byte} and one of them is called a \newterm{bit}).  If a wire has a voltage on it, we say that it encodes a $ 1 $.  If it has no voltage on it, we say that it encodes a $ 0 $.  So, from a set of eight wires (a byte), we interpret a pattern of eight 0's and 1's, e.g., $0 1 0 0 1 0 1 0$.  Using the \newterm{binary} number system, we can interpret this byte as a \newterm{decimal number} (Figure~\ref{fig:byte}).  That's where we come up with the claim that a computer knows about numbers\footnote{We'll talk more about this level of the computer in Chapter~\ref{chap:speed}}. The computer has a \newterm{memory} filled with bytes.  Everything that a computer is working with at a given instant is stored in its memory.  That means that everything that a computer is working with is \emph{encoded} in its bytes: JPEG pictures, Excel spreadsheets, Word documents, annoying Web pop-up ads, and the latest spam email.\begin{figure}\begin{center}\includegraphics{byte.eps}\caption{Eight wires with a pattern of voltages is a byte, which gets interpreted as a pattern of eight 0's and 1's, which gets interpreted as a decimal number.\label{fig:byte}}\end{center}\end{figure}A computer can do lots of things with numbers.  It can add them, subtract them, multiply them, divide them, sort them, collect them, duplicate them, filter them (e.g., ``make a copy of these numbers, but only the even ones''), and compare them and do things based on the comparison.  For example, a computer can be told in a recipe ``Compare these two numbers.  If the first one is less than the second one, jump to step 5 in this recipe.  Otherwise, continue on to the next step.''So far, the computer is an incredible calculator, and that's certainly why it was invented.  The first use of the computer was during World War II for calculating trajectories of projectiles (``If the wind is coming from the SE at 15 MPH, and you want to hit a target 0.5 miles away at an angle of 30 degrees East of North, then incline your launcher to $\ldots$'').  The computer is an amazing calculator.  But what makes it useful for general recipes is the concept of \emph{encodings}.\begin{csconcept}{Computers can layer encodings}Computers can layer encodings to virtually any level of complexity.  Numbers can be interpreted as characters, which can be interpreted in sets as Web pages, which can be interpreted to appear as multiple fonts and styles.  But at the bottommost level, the computer \emph{only} ``knows'' voltages which we intepret as numbers.\end{csconcept}If one of these bytes is interpreted as the number $ 65 $, it could just be the number 65.  Or it could be the letter $ A $ using a standard encoding of numbers-to-letters called the \newterm{American Standard Code for Information Interchange (ASCII)}\index{ASCII}.  If that 65 appears in a collection of other numbers that we're interpreting as text, and that's in a file that ends in ``.html'' it might be part of something that looks like this \code{<a href=$\ldots$}, which a Web browser will interpret as the definition of a link.  Down at the level of the computer, that $ A $ is just a pattern of voltages.  Many layers of recipes up, at the level of a Web browser, it defines something that you can click on to get more information.If the computer understands only numbers (and that's a stretch already), how does it manipulate these encodings?  Sure, it knows how to compare numbers, but how does that extend to being able to alphabetize a class list/  Typically, each layer of encoding is implemented as a piece or layer of software.  There's software that understands how to manipulate characters.  The character software knows how to do things like compare names because it has encoded that $ a $ comes before $ b$ and so on, and that the numeric comparison of the order of numbers in the encoding of the letters leads to alphabetical comparisons.  The character software is used by other software that manipulates text in files.  That's the layer that something like Microsoft Word or Notepad or TextEdit would use.  Still another piece of software knows how to interpret \newterm{HTML} (the language of the Web), and another layer of that software knows how to take HTML and display the right text, fonts, styles, and colors.We can similarly create layers of encodings in the computer for our specific tasks.  We can teach a computer that cells contain mitochondria and DNA, and that DNA has four kinds of nucleotides, and that factories have these kinds of presses and these kinds of stamps.  Creating layers of encoding and interpretation so that the computer is working with the right units (recall back to our recipe analogy) for a given problem is the task of \newterm{data representation} or defining the right \newterm{data structures}.If this sounds like a lot of software, it is.  When software is layered like this, it slows the computer down some.  But the amazing thing about computers is that they're \emph{amazingly} fast---and getting faster all the time!\begin{csconcept}{Moore's Law}\label{sec:moore}Gordon Moore\index{Moore, Gordon}, one of the founders of Intel\index{Intel} (maker of computer processing chips for all computers running Windows operating systems), made the claim that the number of transistors\index{transistor} (a key component of computers) would double at the same price every 18 months, effectively meaning that the same amount of money would buy twice as much computing power every 18 months.  That means, in a year-and-a-half, computers get as fast over again as has taken them since World War II.  This Law has continued to hold true for decades.\end{csconcept}Computers today can execute literally \emph{BILLIONS} of recipe steps per second!  They can hold in memory literally encyclopediae of data!  They never get tired nor bored.  Search a million customers for a particular card holder? No problem!  Find the right set of numbers to get the best value out of an equation? Piece of cake!Process millions of picture elements or sound fragments or movie frames?  That's \newterm{media computation}.\section{Media Computation: Why digitize media?}	\label{sec:mc}Let's consider an encoding that would be appropriate for pictures.  Imagine that pictures were made up of little dots.  That's not hard to imagine: Look really closely at your monitor or at a TV screen and see that your images are \emph{already} made up of little dots.  Each of these dots is a distinct color.  You know from your physics\index{physics!color} that colors can be described as the sum of \newterm{red}, \newterm{green}, and \newterm{blue}.  Add the red and green to get yellow.  Mix all three together to get white.  Turn them all off, and you get a black dot. What if we encoded each dot in a picture as collection of three bytes, one each for the amount of red, green, and blue at that dot on the screen?  And we collect a bunch of these three-byte-sets to determine all the dots of a given picture?  That's a pretty reasonable way of representing pictures, and it's essentially how we're going to do it in Chapter~\ref{chap:encodepict}.  Manipulating these dots (each referred to as a \newterm{pixel} or \newterm{picture element}) can take a lot of processing.  There are thousands or even millions of them in a picture that you might want to work with on your computer or on the Web.  But the computer doesn't get bored and it's mighty fast.The encoding that we will be using for sound involves 44,100 two-byte-sets (called a \newterm{sample}) for each \emph{second} of time.  A three minute song requires 158,760,000 bytes.  Doing any processing on this takes a \emph{lot} of operations.  But at a billion operations per second, you can do lots of operations to every one of those bytes in just a few moments.Creating these kinds of encodings for media requires a change to the media.  Look at the real world: It isn't made up of lots of little dots that you can see.  Listen to a sound: Do you hear thousands of little bits of sound per second?  The fact that you \emph{can't} hear little bits of sound per second is what makes it possible to create these encodings.  Our eyes and ears are limited: We can only perceive so much, and only things that are just so small.  If you break up an image into small enough dots, your eyes can't tell that it's not a continuous flow of color.  If you break up a sound into small enough pieces, your ears can't tell that the sound isn't a continuous flow of auditory energy.The process of encoding media into little bits is called \newterm{digitization}, sometimes referred to as ``\newterm{going digital}.''  \newterm{Digital} means (according to the American Heritage Dictionary) ``Of, relating to, or resembling a digit, especially a finger.'' Making things digital is about turning things from continuous, uncountable, to something that we can count, as if with our fingers.\newterm{Digital media}, done well, feel the same to our limited human sensory apparatus as the original.  Phonograph recordings (ever seen one of those?) capture sound continuously, as an \newterm{analogue} signal.  Photographs capture light as a continuous flow.  Some people say that they can hear a difference between phonograph recordings and CD recordings, but to my ear and most measurements, a CD (which \emph{is} digitized sound) sounds just the same---maybe clearer.  Digital cameras at high enough resolutions produce photograph-quality pictures.Why would you want to digitize media\index{digitizing media!why?}?  Because it's easier to manipulate, to replicate exactly, to compress, and to transmit.  For example, it's hard to manipulate images that are in photographs, but it's very easy when the same images are digitized.  This book is about using the increasingly digital world of media and manipulating it---and learning computation in the process.Moore's Law has made media computation feasible as an introductory topic. Media computation relies on the computer doing lots and lots of operations on lots and lots of bytes.  Modern computers can do this easily. Even with slow (but easy to understand) languages, even with inefficient (but easy to read and write) recipes, we can learn about computation by manipulating media.  \section{Computer Science for Non-Computer Scientists} %Why should normal people learn it?	But why should you?  Why should anyone who doesn't want to be a computer scientist learn about computer science?  Why should you be interested in learning about computation through manipulating media?Most professionals today do manipulate media: Papers, videos, tape recordings, photographs, drawings.  Increasingly, this manipulation is done with a computer.  Media are very often in a digitized form today.  We use software to manipulate these media.  We use Adobe Photoshop for manipulating our images, and Macromedia SoundEdit to manipulate our sounds, and perhaps Microsoft PowerPoint for assembling our media into slideshows.  We use Microsoft Word for manipulating our text, and Netscape Navigator or Microsoft Internet Explorer for browsing media on the Internet.So why should anyone who does \emph{not} want to be a computer scientist study computer science?  Why should you learn to program?  Isn't it enough to learn to \emph{use} all this great software?  The following two sections provide two answers to these questions.\subsection{It's about communication} %The communication argument goes here	Digital media are manipulated with software.  \emph{If you can only manipulate media with software that \textbf{someone else} made for you, you are limiting your ability to communicate.}  What if you want to say something or say it in some way that Adobe, Microsoft, Apple, and the rest don't support you in saying?  If you know how to program, even if it would take you \emph{longer} to do it yourself, you have that freedom.What about learning those tools in the first place?  In my years in computers, I've seen a variety of software come and go as \emph{the} package for drawing, painting, word-processing, video editing, and beyond.  You can't learn just a single tool and expect to be able to use that your entire career.  If you know \emph{how} the tools work, you have a core understanding that can transfer from tool to tool.  You can think about your media work in terms of the \emph{algorithms}, not the \emph{tools}.Finally, if you're going to prepare media for the Web, for marketing, for print, for broadcast, for any use whatsoever, it's worthwhile for you to have a sense of what's possible, what can be done with media.  It's even more important as a consumer of media that you know how the media can be manipulated, to know what's true and what could be just a trick.  If you know the basics of media computation, you have an understanding that goes beyond what any individual tool provides.\subsection{It's about process}	\index{Perlis, Alan}\index{software engineering}In 1961, Alan Perlis gave a talk at MIT where he made the argument that computer science, and programming explicitly, should be part of a general, liberal education~\cite{greenberger}.  Perlis is an important figure in the field of computer science (Figure~\ref{fig:perlis}).  The highest award that a computer scientist can be honored with is the ACM Turing Award.  Perlis was the first recipient of that award.  He's an important figure in software engineering, and he started several of the first computer science departments in the United States.\index{ACM Turing Award}\index{liberal education}\begin{figure}\begin{center}\includegraphics{perlis.eps}\end{center}\caption{Alan Perlis\label{fig:perlis}}\end{figure}\index{calculus}\index{process} Perlis' argument was made in comparison with calculus.  Calculus is generally considered part of a liberal education: Not \emph{everyone} takes calculus, but if you want to be well-educated, you will typically take at least a term of calculus.  Calculus is the study of \emph{rates}, which is important in many fields.  Computer science, as we said before (page~\pageref{pg:process}), is the study of \emph{process}.  Process is important to nearly every field, from business to science to medicine to law.  Knowing process formally is important to everyone.\section*{Exercises}	\exercise Find an ASCII table on the Web: A table listing every character and its corresponding numeric representation.\exercise Find a \newterm{Unicode} table on the Web.  What's the difference between ASCII and Unicode?\exercise Consider the representation for pictures described in Section~\ref{sec:mc}, where each ``dot'' (pixel) in the picture is represented by three bytes, for the red, green, and blue components of the color at that dot.  How many bytes does it take to represent a $ 640 x 480 $ picture, a common picture size on the Web?  How many bytes does it take to represent a $1024 x 768 $ picture, a common screen size?  (What do you think is meant now by a ``3 megapixel'' camera?)\exercise How many different numbers can be represented by one byte?  In other words, eight bits can represent from zero to what number?  What if you have two bytes?  Four bytes?\exercise (Hard) How might you represent a \newterm{floating point number} in terms of bytes?\exercise Look up Alan Kay and the \newterm{Dynabook} on the Web.  Who is he, and what does he have to do with media computation?\exercise Look up Alan Turing on the Web.  Who was he, and what does he have to do with our notion of what a computer can do and how encodings work?\exercise Look up Kurt Goedel on the Web.  Who was he, and what amazing things did he do with encodings?\section*{To Dig Deeper}	James Gleick's book \emph{Chaos} describes more on emergent properties.Mitchel Resnick's book \emph{Turtles, Termites, and Traffic Jams: Explorations in Massively Parallel Microworlds}~\cite{starlogo} describes how ants, termites, and even traffic jams and slime molds can be described pretty accurately with hundreds or thousands of very small programs running and interacting all at once.\emph{Beyond the Digital Domain}~\cite{digitaldomain} is a wonderful introductory book to computation with lots of good information about digital media.